{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxL9N8noxD4X"
      },
      "source": [
        "**Question 1: Foundational Graph Theorybold text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffmUKXOevzp7",
        "outputId": "6dbb4f28-b192-4a60-9e96-a5d872b0485e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjacency Matrix:\n",
            " [[0 1 1 0 0]\n",
            " [1 0 1 1 0]\n",
            " [1 1 0 1 1]\n",
            " [0 1 1 0 1]\n",
            " [0 0 1 1 0]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "edges = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (2, 4), (3, 4)]\n",
        "G = nx.Graph(edges)\n",
        "\n",
        "# Adjacency Matrix\n",
        "A = nx.adjacency_matrix(G).todense()\n",
        "print(\"Adjacency Matrix:\\n\", A)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVTEJX48wdk1",
        "outputId": "e99c8fcd-0fe9-44ca-d7c2-dd0b4050305a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Degree Matrix:\n",
            " [[2 0 0 0 0]\n",
            " [0 3 0 0 0]\n",
            " [0 0 4 0 0]\n",
            " [0 0 0 3 0]\n",
            " [0 0 0 0 2]]\n"
          ]
        }
      ],
      "source": [
        "D = np.diag([d for _, d in G.degree()])\n",
        "print(\"Degree Matrix:\\n\", D)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7zLHQKOwovz",
        "outputId": "2f8c3b34-5fba-469b-a791-8d1c4800a543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laplacian Matrix:\n",
            " [[ 2 -1 -1  0  0]\n",
            " [-1  3 -1 -1  0]\n",
            " [-1 -1  4 -1 -1]\n",
            " [ 0 -1 -1  3 -1]\n",
            " [ 0  0 -1 -1  2]]\n"
          ]
        }
      ],
      "source": [
        "L = D - A\n",
        "print(\"Laplacian Matrix:\\n\", L)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dklMTc-ywtRd",
        "outputId": "56f70b22-c31c-4e42-add9-8af7ff6773f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Laplacian:\n",
            " [[ 1.         -0.40824829 -0.35355339  0.          0.        ]\n",
            " [-0.40824829  1.         -0.28867513 -0.33333333  0.        ]\n",
            " [-0.35355339 -0.28867513  1.         -0.28867513 -0.35355339]\n",
            " [ 0.         -0.33333333 -0.28867513  1.         -0.40824829]\n",
            " [ 0.          0.         -0.35355339 -0.40824829  1.        ]]\n"
          ]
        }
      ],
      "source": [
        "D_inv_sqrt = np.diag(1.0 / np.sqrt(np.diag(D)))\n",
        "L_sym = np.eye(len(G)) - D_inv_sqrt @ A @ D_inv_sqrt\n",
        "print(\"Normalized Laplacian:\\n\", L_sym)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BkTrttFwyLf",
        "outputId": "f4d9ac06-8ae0-4a90-a92d-0e24be999c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalues:\n",
            " [0.         1.58578644 3.         5.         4.41421356]\n"
          ]
        }
      ],
      "source": [
        "eigenvalues = np.linalg.eigvals(L)\n",
        "print(\"Eigenvalues:\\n\", eigenvalues)\n",
        "assert np.all(eigenvalues >= 0), \"Eigenvalues should be non-negative!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "YM8p2vY7wzHs",
        "outputId": "bb5c3a60-02f7-4c94-d75a-bd2b9e2501a0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "degree_sequence = [d for _, d in G.degree()]\n",
        "plt.hist(degree_sequence, bins=np.arange(min(degree_sequence), max(degree_sequence) + 2))\n",
        "plt.title(\"Degree Distribution\")\n",
        "plt.xlabel(\"Degree\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSB9z2vOxUWX"
      },
      "source": [
        "**Question 2: GCN and ChebNet Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install torch-geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGzYHUxUQhVe",
        "outputId": "f8898c84-74cf-4d47-bdca-a2ef4a5c8aca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.A"
      ],
      "metadata": {
        "id": "cjNU2Cpla1uO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Train a GCN on the Cora dataset using the GCNConv layer from the\n",
        "torch geometric.nn library.**\n",
        "*  **Report the training accuracy, validation\n",
        "accuracy, and test accuracy**"
      ],
      "metadata": {
        "id": "YI5Mlq3KiA1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "# Load Cora dataset\n",
        "dataset = Planetoid(root=\"/tmp/Cora\", name=\"Cora\")\n",
        "data = dataset[0]\n",
        "\n",
        "# Define GCN model\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Model, optimizer, and loss function\n",
        "model = GCN()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "# Training loop\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Test function\n",
        "def test():\n",
        "    model.eval()\n",
        "    out = model(data)\n",
        "    pred = out.argmax(dim=1)\n",
        "    accs = []\n",
        "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
        "        correct = (pred[mask] == data.y[mask]).sum()\n",
        "        accs.append(int(correct) / int(mask.sum()))\n",
        "    return accs\n",
        "\n",
        "# Run training and testing\n",
        "for epoch in range(100):\n",
        "    loss = train()\n",
        "    train_acc, val_acc, test_acc = test()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "print(f\"Final Results - Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tU38ZRyhasOy",
        "outputId": "efe68e21-3158-4728-933f-023a5411a314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 000, Loss: 1.9440, Train Acc: 0.6286, Val Acc: 0.3640, Test Acc: 0.3760\n",
            "Epoch 010, Loss: 0.7084, Train Acc: 0.9786, Val Acc: 0.7540, Test Acc: 0.7840\n",
            "Epoch 020, Loss: 0.1984, Train Acc: 1.0000, Val Acc: 0.7720, Test Acc: 0.7950\n",
            "Epoch 030, Loss: 0.1231, Train Acc: 1.0000, Val Acc: 0.7600, Test Acc: 0.7790\n",
            "Epoch 040, Loss: 0.0760, Train Acc: 1.0000, Val Acc: 0.7600, Test Acc: 0.7880\n",
            "Epoch 050, Loss: 0.0460, Train Acc: 1.0000, Val Acc: 0.7740, Test Acc: 0.8040\n",
            "Epoch 060, Loss: 0.0406, Train Acc: 1.0000, Val Acc: 0.7620, Test Acc: 0.7980\n",
            "Epoch 070, Loss: 0.0409, Train Acc: 1.0000, Val Acc: 0.7720, Test Acc: 0.7990\n",
            "Epoch 080, Loss: 0.0358, Train Acc: 1.0000, Val Acc: 0.7680, Test Acc: 0.8010\n",
            "Epoch 090, Loss: 0.0522, Train Acc: 1.0000, Val Acc: 0.7700, Test Acc: 0.8130\n",
            "Final Results - Train Acc: 1.0000, Val Acc: 0.7620, Test Acc: 0.8120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6RASYbJJco5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. B"
      ],
      "metadata": {
        "id": "OiF3tTbFUXMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  **Train a ChebNet on the Cora dataset using the ChebConv layer from the\n",
        "torch geometric.nn library.**\n",
        "*  **Report the training accuracy, validation\n",
        "accuracy, and test accuracy**"
      ],
      "metadata": {
        "id": "BitmBBP5iRDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import ChebConv\n",
        "\n",
        "# Load the Cora dataset\n",
        "dataset = Planetoid(root=\"/tmp/Cora\", name=\"Cora\")\n",
        "data = dataset[0]\n",
        "\n",
        "# Define ChebNet model\n",
        "class ChebNet(torch.nn.Module):\n",
        "    def __init__(self, K=3):  # K is the order of the Chebyshev polynomial\n",
        "        super(ChebNet, self).__init__()\n",
        "        self.conv1 = ChebConv(dataset.num_node_features, 16, K)\n",
        "        self.conv2 = ChebConv(16, dataset.num_classes, K)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Instantiate the model, optimizer, and loss function\n",
        "model = ChebNet(K=3)  # Set K (order of Chebyshev polynomial) to 3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "# Training function\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Testing function\n",
        "def test():\n",
        "    model.eval()\n",
        "    out = model(data)\n",
        "    pred = out.argmax(dim=1)\n",
        "    accs = []\n",
        "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
        "        correct = (pred[mask] == data.y[mask]).sum()\n",
        "        accs.append(int(correct) / int(mask.sum()))\n",
        "    return accs\n",
        "\n",
        "# Train and evaluate\n",
        "for epoch in range(200):  # Train for 200 epochs\n",
        "    loss = train()\n",
        "    train_acc, val_acc, test_acc = test()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "# Final results\n",
        "print(f\"Final Results - Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZwDfg5la9iL",
        "outputId": "862323b4-84af-4ae8-e7e2-0f62d6fd0669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 000, Loss: 1.9376, Train Acc: 0.7357, Val Acc: 0.2440, Test Acc: 0.2510\n",
            "Epoch 010, Loss: 0.1102, Train Acc: 1.0000, Val Acc: 0.6880, Test Acc: 0.7180\n",
            "Epoch 020, Loss: 0.0259, Train Acc: 1.0000, Val Acc: 0.6900, Test Acc: 0.7220\n",
            "Epoch 030, Loss: 0.0140, Train Acc: 1.0000, Val Acc: 0.7120, Test Acc: 0.7300\n",
            "Epoch 040, Loss: 0.0065, Train Acc: 1.0000, Val Acc: 0.7180, Test Acc: 0.7390\n",
            "Epoch 050, Loss: 0.0178, Train Acc: 1.0000, Val Acc: 0.7140, Test Acc: 0.7420\n",
            "Epoch 060, Loss: 0.0031, Train Acc: 1.0000, Val Acc: 0.7120, Test Acc: 0.7470\n",
            "Epoch 070, Loss: 0.0232, Train Acc: 1.0000, Val Acc: 0.7200, Test Acc: 0.7570\n",
            "Epoch 080, Loss: 0.0097, Train Acc: 1.0000, Val Acc: 0.7200, Test Acc: 0.7580\n",
            "Epoch 090, Loss: 0.0128, Train Acc: 1.0000, Val Acc: 0.7060, Test Acc: 0.7540\n",
            "Epoch 100, Loss: 0.0035, Train Acc: 1.0000, Val Acc: 0.7260, Test Acc: 0.7560\n",
            "Epoch 110, Loss: 0.0083, Train Acc: 1.0000, Val Acc: 0.7440, Test Acc: 0.7760\n",
            "Epoch 120, Loss: 0.0097, Train Acc: 1.0000, Val Acc: 0.7360, Test Acc: 0.7710\n",
            "Epoch 130, Loss: 0.0086, Train Acc: 1.0000, Val Acc: 0.7420, Test Acc: 0.7600\n",
            "Epoch 140, Loss: 0.0164, Train Acc: 1.0000, Val Acc: 0.7440, Test Acc: 0.7620\n",
            "Epoch 150, Loss: 0.0056, Train Acc: 1.0000, Val Acc: 0.7520, Test Acc: 0.7640\n",
            "Epoch 160, Loss: 0.0071, Train Acc: 1.0000, Val Acc: 0.7580, Test Acc: 0.7740\n",
            "Epoch 170, Loss: 0.0089, Train Acc: 1.0000, Val Acc: 0.7560, Test Acc: 0.7730\n",
            "Epoch 180, Loss: 0.0072, Train Acc: 1.0000, Val Acc: 0.7540, Test Acc: 0.7680\n",
            "Epoch 190, Loss: 0.0094, Train Acc: 1.0000, Val Acc: 0.7700, Test Acc: 0.7880\n",
            "Final Results - Train Acc: 1.0000, Val Acc: 0.7540, Test Acc: 0.7740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. C"
      ],
      "metadata": {
        "id": "A4qqqYkUT-YI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy Results:**\n",
        "\n",
        "*  GCN: Typically performs well on small datasets like Cora, achieving high test accuracy due to its simple architecture and ability to capture node representations effectively.\n",
        "*   ChebNet: Often matches or slightly exceeds GCN's performance on tasks with smooth node features because it incorporates higher-order graph structure through Chebyshev polynomials."
      ],
      "metadata": {
        "id": "hnZnSQT0Skrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observed Differences:\n",
        "\n",
        "* GCN:\n",
        "Easier to train and tune due to fewer hyperparameters.\n",
        "Relies on first-order neighbors, so it might miss long-range dependencies.\n",
        "ChebNet:\n",
        "More flexible with the parameter K (polynomial order), allowing for control over neighborhood size.\n",
        "Captures higher-order relationships but is computationally more intensive for larger K.\n",
        "Insights:\n",
        "\n",
        "* For the Cora dataset, GCN and ChebNet often achieve similar performance as the graph is small and well-structured.\n",
        "ChebNet may outperform GCN slightly when K is optimized, as it captures global graph structure better.\n",
        "Discussion Points:\n",
        "Use GCN for simpler tasks or smaller datasets due to its efficiency.\n",
        "Use ChebNet when capturing higher-order relationships is crucial, especially in larger graphs."
      ],
      "metadata": {
        "id": "lD0GzzAkVKNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. D"
      ],
      "metadata": {
        "id": "yej4zTK2WBW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "Hyperparameter Variations:\n",
        "\n",
        "* Order\n",
        "𝐾\n",
        "K: Controls the range of neighbor nodes captured by ChebNet.\n",
        "Hidden Layer Size: Adjusts the capacity of the network to learn node embeddings.\n",
        "Training Loop:\n",
        "\n",
        "* Trains the model for 200 epochs.\n",
        "Logs performance every 50 epochs for monitoring.\n",
        "Results:\n",
        "\n",
        "* Prints train, validation, and test accuracies for each experiment.\n",
        "Summarizes all results for easier comparison."
      ],
      "metadata": {
        "id": "k1X_d0p4bAPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import ChebConv\n",
        "\n",
        "# Load Cora dataset\n",
        "dataset = Planetoid(root=\"/tmp/Cora\", name=\"Cora\")\n",
        "data = dataset[0]\n",
        "\n",
        "# Define ChebNet model with flexible hyperparameters\n",
        "class ChebNet(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim=16, K=3):\n",
        "        super(ChebNet, self).__init__()\n",
        "        self.conv1 = ChebConv(dataset.num_node_features, hidden_dim, K)\n",
        "        self.conv2 = ChebConv(hidden_dim, dataset.num_classes, K)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Training function\n",
        "def train(model, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Testing function\n",
        "def test(model):\n",
        "    model.eval()\n",
        "    out = model(data)\n",
        "    pred = out.argmax(dim=1)\n",
        "    accs = []\n",
        "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
        "        correct = (pred[mask] == data.y[mask]).sum()\n",
        "        accs.append(int(correct) / int(mask.sum()))\n",
        "    return accs\n",
        "\n",
        "# Experiment with different hyperparameters\n",
        "results = []\n",
        "for K in [2, 3, 5,8]:  # Orders of the Chebyshev polynomial\n",
        "    for hidden_dim in [16, 32, 64,128]:  # Hidden layer sizes\n",
        "        print(f\"\\nExperimenting with K={K}, Hidden Dim={hidden_dim}\")\n",
        "        model = ChebNet(hidden_dim=hidden_dim, K=K)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "        # Train the model\n",
        "        for epoch in range(200):  # Train for 200 epochs\n",
        "            loss = train(model, optimizer)\n",
        "            if epoch % 50 == 0:\n",
        "                train_acc, val_acc, test_acc = test(model)\n",
        "                print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "        # Final test accuracies\n",
        "        train_acc, val_acc, test_acc = test(model)\n",
        "        results.append((K, hidden_dim, train_acc, val_acc, test_acc))\n",
        "        print(f\"Final Results - K={K}, Hidden Dim={hidden_dim}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "# Display all results\n",
        "print(\"\\nSummary of Experiments:\")\n",
        "print(\"K\\tHidden Dim\\tTrain Acc\\tVal Acc\\t\\tTest Acc\")\n",
        "for K, hidden_dim, train_acc, val_acc, test_acc in results:\n",
        "    print(f\"{K}\\t{hidden_dim}\\t\\t{train_acc:.4f}\\t\\t{val_acc:.4f}\\t\\t{test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQTkl0_tQwzg",
        "outputId": "24295b6d-a027-4583-b45e-10dd6d364696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experimenting with K=2, Hidden Dim=16\n",
            "Epoch 000, Loss: 1.9892, Train Acc: 0.5500, Val Acc: 0.3000, Test Acc: 0.3160\n",
            "Epoch 050, Loss: 0.0291, Train Acc: 1.0000, Val Acc: 0.7620, Test Acc: 0.7830\n",
            "Epoch 100, Loss: 0.0157, Train Acc: 1.0000, Val Acc: 0.7800, Test Acc: 0.8010\n",
            "Epoch 150, Loss: 0.0307, Train Acc: 1.0000, Val Acc: 0.7880, Test Acc: 0.8060\n",
            "Final Results - K=2, Hidden Dim=16, Train Acc: 1.0000, Val Acc: 0.7740, Test Acc: 0.8000\n",
            "\n",
            "Experimenting with K=2, Hidden Dim=32\n",
            "Epoch 000, Loss: 1.9812, Train Acc: 0.9071, Val Acc: 0.4600, Test Acc: 0.4670\n",
            "Epoch 050, Loss: 0.0099, Train Acc: 1.0000, Val Acc: 0.7660, Test Acc: 0.7750\n",
            "Epoch 100, Loss: 0.0102, Train Acc: 1.0000, Val Acc: 0.7640, Test Acc: 0.7920\n",
            "Epoch 150, Loss: 0.0113, Train Acc: 1.0000, Val Acc: 0.7760, Test Acc: 0.7900\n",
            "Final Results - K=2, Hidden Dim=32, Train Acc: 1.0000, Val Acc: 0.7780, Test Acc: 0.7940\n",
            "\n",
            "Experimenting with K=2, Hidden Dim=64\n",
            "Epoch 000, Loss: 2.0038, Train Acc: 0.9286, Val Acc: 0.4780, Test Acc: 0.5030\n",
            "Epoch 050, Loss: 0.0024, Train Acc: 1.0000, Val Acc: 0.7520, Test Acc: 0.7550\n",
            "Epoch 100, Loss: 0.0063, Train Acc: 1.0000, Val Acc: 0.7620, Test Acc: 0.7810\n",
            "Epoch 150, Loss: 0.0044, Train Acc: 1.0000, Val Acc: 0.7720, Test Acc: 0.7890\n",
            "Final Results - K=2, Hidden Dim=64, Train Acc: 1.0000, Val Acc: 0.7780, Test Acc: 0.7940\n",
            "\n",
            "Experimenting with K=2, Hidden Dim=128\n",
            "Epoch 000, Loss: 1.9760, Train Acc: 0.9714, Val Acc: 0.4500, Test Acc: 0.5060\n",
            "Epoch 050, Loss: 0.0038, Train Acc: 1.0000, Val Acc: 0.7380, Test Acc: 0.7620\n",
            "Epoch 100, Loss: 0.0037, Train Acc: 1.0000, Val Acc: 0.7560, Test Acc: 0.7800\n",
            "Epoch 150, Loss: 0.0031, Train Acc: 1.0000, Val Acc: 0.7460, Test Acc: 0.7780\n",
            "Final Results - K=2, Hidden Dim=128, Train Acc: 1.0000, Val Acc: 0.7580, Test Acc: 0.7800\n",
            "\n",
            "Experimenting with K=3, Hidden Dim=16\n",
            "Epoch 000, Loss: 2.0004, Train Acc: 0.7429, Val Acc: 0.3660, Test Acc: 0.3860\n",
            "Epoch 050, Loss: 0.0115, Train Acc: 1.0000, Val Acc: 0.7500, Test Acc: 0.7580\n",
            "Epoch 100, Loss: 0.0247, Train Acc: 1.0000, Val Acc: 0.7700, Test Acc: 0.7660\n",
            "Epoch 150, Loss: 0.0062, Train Acc: 1.0000, Val Acc: 0.7740, Test Acc: 0.7820\n",
            "Final Results - K=3, Hidden Dim=16, Train Acc: 1.0000, Val Acc: 0.7860, Test Acc: 0.7930\n",
            "\n",
            "Experimenting with K=3, Hidden Dim=32\n",
            "Epoch 000, Loss: 2.0370, Train Acc: 0.8643, Val Acc: 0.3680, Test Acc: 0.3530\n",
            "Epoch 050, Loss: 0.0019, Train Acc: 1.0000, Val Acc: 0.7420, Test Acc: 0.7830\n",
            "Epoch 100, Loss: 0.0058, Train Acc: 1.0000, Val Acc: 0.7660, Test Acc: 0.7980\n",
            "Epoch 150, Loss: 0.0040, Train Acc: 1.0000, Val Acc: 0.7820, Test Acc: 0.7940\n",
            "Final Results - K=3, Hidden Dim=32, Train Acc: 1.0000, Val Acc: 0.7820, Test Acc: 0.8050\n",
            "\n",
            "Experimenting with K=3, Hidden Dim=64\n",
            "Epoch 000, Loss: 1.9937, Train Acc: 0.9857, Val Acc: 0.5040, Test Acc: 0.5260\n",
            "Epoch 050, Loss: 0.0013, Train Acc: 1.0000, Val Acc: 0.7600, Test Acc: 0.7880\n",
            "Epoch 100, Loss: 0.0025, Train Acc: 1.0000, Val Acc: 0.7880, Test Acc: 0.8050\n",
            "Epoch 150, Loss: 0.0030, Train Acc: 1.0000, Val Acc: 0.7840, Test Acc: 0.8110\n",
            "Final Results - K=3, Hidden Dim=64, Train Acc: 1.0000, Val Acc: 0.8080, Test Acc: 0.8030\n",
            "\n",
            "Experimenting with K=3, Hidden Dim=128\n",
            "Epoch 000, Loss: 2.0600, Train Acc: 0.9571, Val Acc: 0.4660, Test Acc: 0.5070\n",
            "Epoch 050, Loss: 0.0017, Train Acc: 1.0000, Val Acc: 0.7280, Test Acc: 0.7340\n",
            "Epoch 100, Loss: 0.0027, Train Acc: 1.0000, Val Acc: 0.7780, Test Acc: 0.8050\n",
            "Epoch 150, Loss: 0.0024, Train Acc: 1.0000, Val Acc: 0.7700, Test Acc: 0.8010\n",
            "Final Results - K=3, Hidden Dim=128, Train Acc: 1.0000, Val Acc: 0.7760, Test Acc: 0.7990\n",
            "\n",
            "Experimenting with K=5, Hidden Dim=16\n",
            "Epoch 000, Loss: 2.1061, Train Acc: 0.7857, Val Acc: 0.2260, Test Acc: 0.2050\n",
            "Epoch 050, Loss: 0.0072, Train Acc: 1.0000, Val Acc: 0.6320, Test Acc: 0.6320\n",
            "Epoch 100, Loss: 0.0043, Train Acc: 1.0000, Val Acc: 0.6680, Test Acc: 0.6540\n",
            "Epoch 150, Loss: 0.0068, Train Acc: 1.0000, Val Acc: 0.7220, Test Acc: 0.7220\n",
            "Final Results - K=5, Hidden Dim=16, Train Acc: 1.0000, Val Acc: 0.7300, Test Acc: 0.7460\n",
            "\n",
            "Experimenting with K=5, Hidden Dim=32\n",
            "Epoch 000, Loss: 2.0777, Train Acc: 0.9429, Val Acc: 0.3620, Test Acc: 0.3590\n",
            "Epoch 050, Loss: 0.0060, Train Acc: 1.0000, Val Acc: 0.6940, Test Acc: 0.6750\n",
            "Epoch 100, Loss: 0.0024, Train Acc: 1.0000, Val Acc: 0.7340, Test Acc: 0.7430\n",
            "Epoch 150, Loss: 0.0028, Train Acc: 1.0000, Val Acc: 0.7580, Test Acc: 0.7590\n",
            "Final Results - K=5, Hidden Dim=32, Train Acc: 1.0000, Val Acc: 0.7660, Test Acc: 0.7830\n",
            "\n",
            "Experimenting with K=5, Hidden Dim=64\n",
            "Epoch 000, Loss: 2.1351, Train Acc: 0.8714, Val Acc: 0.2480, Test Acc: 0.2810\n",
            "Epoch 050, Loss: 0.0010, Train Acc: 1.0000, Val Acc: 0.7460, Test Acc: 0.7410\n",
            "Epoch 100, Loss: 0.0038, Train Acc: 1.0000, Val Acc: 0.7460, Test Acc: 0.7510\n",
            "Epoch 150, Loss: 0.0033, Train Acc: 1.0000, Val Acc: 0.7620, Test Acc: 0.7670\n",
            "Final Results - K=5, Hidden Dim=64, Train Acc: 1.0000, Val Acc: 0.7760, Test Acc: 0.7760\n",
            "\n",
            "Experimenting with K=5, Hidden Dim=128\n",
            "Epoch 000, Loss: 2.1329, Train Acc: 0.9071, Val Acc: 0.4600, Test Acc: 0.4650\n",
            "Epoch 050, Loss: 0.0011, Train Acc: 1.0000, Val Acc: 0.6920, Test Acc: 0.7020\n",
            "Epoch 100, Loss: 0.0014, Train Acc: 1.0000, Val Acc: 0.7120, Test Acc: 0.7270\n",
            "Epoch 150, Loss: 0.0019, Train Acc: 1.0000, Val Acc: 0.7440, Test Acc: 0.7630\n",
            "Final Results - K=5, Hidden Dim=128, Train Acc: 1.0000, Val Acc: 0.7600, Test Acc: 0.7670\n",
            "\n",
            "Experimenting with K=8, Hidden Dim=16\n",
            "Epoch 000, Loss: 2.2053, Train Acc: 0.7643, Val Acc: 0.2140, Test Acc: 0.1990\n",
            "Epoch 050, Loss: 0.0017, Train Acc: 1.0000, Val Acc: 0.4980, Test Acc: 0.4870\n",
            "Epoch 100, Loss: 0.0166, Train Acc: 1.0000, Val Acc: 0.5680, Test Acc: 0.5510\n",
            "Epoch 150, Loss: 0.0186, Train Acc: 1.0000, Val Acc: 0.6120, Test Acc: 0.6090\n",
            "Final Results - K=8, Hidden Dim=16, Train Acc: 1.0000, Val Acc: 0.6900, Test Acc: 0.6660\n",
            "\n",
            "Experimenting with K=8, Hidden Dim=32\n",
            "Epoch 000, Loss: 2.4866, Train Acc: 0.4714, Val Acc: 0.3220, Test Acc: 0.3420\n",
            "Epoch 050, Loss: 0.0034, Train Acc: 1.0000, Val Acc: 0.5280, Test Acc: 0.5150\n",
            "Epoch 100, Loss: 0.0049, Train Acc: 1.0000, Val Acc: 0.6280, Test Acc: 0.6080\n",
            "Epoch 150, Loss: 0.0019, Train Acc: 1.0000, Val Acc: 0.6460, Test Acc: 0.6270\n",
            "Final Results - K=8, Hidden Dim=32, Train Acc: 1.0000, Val Acc: 0.6740, Test Acc: 0.6660\n",
            "\n",
            "Experimenting with K=8, Hidden Dim=64\n",
            "Epoch 000, Loss: 2.5145, Train Acc: 0.9357, Val Acc: 0.2180, Test Acc: 0.2060\n",
            "Epoch 050, Loss: 0.0013, Train Acc: 1.0000, Val Acc: 0.5500, Test Acc: 0.5390\n",
            "Epoch 100, Loss: 0.0015, Train Acc: 1.0000, Val Acc: 0.7260, Test Acc: 0.7030\n",
            "Epoch 150, Loss: 0.0036, Train Acc: 1.0000, Val Acc: 0.6920, Test Acc: 0.7010\n",
            "Final Results - K=8, Hidden Dim=64, Train Acc: 1.0000, Val Acc: 0.7320, Test Acc: 0.7500\n",
            "\n",
            "Experimenting with K=8, Hidden Dim=128\n",
            "Epoch 000, Loss: 2.4626, Train Acc: 0.7571, Val Acc: 0.2400, Test Acc: 0.2170\n",
            "Epoch 050, Loss: 0.0020, Train Acc: 1.0000, Val Acc: 0.4960, Test Acc: 0.5100\n",
            "Epoch 100, Loss: 0.0014, Train Acc: 1.0000, Val Acc: 0.6520, Test Acc: 0.6460\n",
            "Epoch 150, Loss: 0.0015, Train Acc: 1.0000, Val Acc: 0.7040, Test Acc: 0.7080\n",
            "Final Results - K=8, Hidden Dim=128, Train Acc: 1.0000, Val Acc: 0.7180, Test Acc: 0.7190\n",
            "\n",
            "Summary of Experiments:\n",
            "K\tHidden Dim\tTrain Acc\tVal Acc\t\tTest Acc\n",
            "2\t16\t\t1.0000\t\t0.7740\t\t0.8000\n",
            "2\t32\t\t1.0000\t\t0.7780\t\t0.7940\n",
            "2\t64\t\t1.0000\t\t0.7780\t\t0.7940\n",
            "2\t128\t\t1.0000\t\t0.7580\t\t0.7800\n",
            "3\t16\t\t1.0000\t\t0.7860\t\t0.7930\n",
            "3\t32\t\t1.0000\t\t0.7820\t\t0.8050\n",
            "3\t64\t\t1.0000\t\t0.8080\t\t0.8030\n",
            "3\t128\t\t1.0000\t\t0.7760\t\t0.7990\n",
            "5\t16\t\t1.0000\t\t0.7300\t\t0.7460\n",
            "5\t32\t\t1.0000\t\t0.7660\t\t0.7830\n",
            "5\t64\t\t1.0000\t\t0.7760\t\t0.7760\n",
            "5\t128\t\t1.0000\t\t0.7600\t\t0.7670\n",
            "8\t16\t\t1.0000\t\t0.6900\t\t0.6660\n",
            "8\t32\t\t1.0000\t\t0.6740\t\t0.6660\n",
            "8\t64\t\t1.0000\t\t0.7320\t\t0.7500\n",
            "8\t128\t\t1.0000\t\t0.7180\t\t0.7190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qMlGt-L9WEH0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}